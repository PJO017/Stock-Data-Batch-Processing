{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 00:37:36 WARN Utils: Your hostname, PJ-Ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.251 instead (on interface wlo1)\n",
      "23/04/16 00:37:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/16 00:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Stock-Data-Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.251:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Stock-Data-Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbb109f9300>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset \n",
    "df = spark.read.option('header', 'true').csv(\"./Data/FS_sp500_Value.csv\").drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis functions \n",
    "\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def calcSimpleMovingAvg(df, col, span):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-span, 0)\n",
    "  df = df.withColumn(\"moving_avg\", F.avg(col).over(window))\n",
    "\n",
    "  return df.rdd.map(lambda x: x.moving_avg).collect()\n",
    "\n",
    "def calcDailyPercentChange(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"change\", F.when(F.isnull( (df.Close - df.prev_close)/df.prev_close ), 0).otherwise(F.round(F.abs((df.Close - df.prev_close)/df.prev_close) *100, 3)) )\n",
    "\n",
    "  return df.rdd.map(lambda x: x.change).collect()\n",
    "\n",
    "def calcATR(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"h-l\", df.High-df.Low)\n",
    "  df = df.withColumn(\"h-p\", F.when(F.isnull( F.abs(df.High-df.prev_close)), 0).otherwise( F.abs(df.High-df.prev_close)))\n",
    "  df = df.withColumn(\"l-p\", F.when(F.isnull( F.abs(df.Low-df.prev_close)), 0).otherwise( F.abs(df.Low-df.prev_close)))\n",
    "  df = df.withColumn(\"true_range\", F.greatest(\"h-l\", \"h-p\", \"l-p\"))\n",
    "\n",
    "  return calcSimpleMovingAvg(df, \"true_range\", 14)\n",
    "\n",
    "def calcRSI(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window)) \n",
    "  df = df.withColumn(\"change\", F.when(F.isnull(df.Close - df.prev_close), 0).otherwise(df.Close - df.prev_close))\n",
    "\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-14, 0)\n",
    "  df = df.withColumn(\"change_up\", F.when(df.change < 0, 0).otherwise(df.change))\n",
    "  df = df.withColumn(\"change_down\", F.when(df.change > 0, 0).otherwise(df.change))\n",
    "  df = df.withColumn(\"avg_up\", F.avg(df.change_up).over(window)).drop(df.change_up)\n",
    "  df = df.withColumn(\"avg_down\", F.avg(df.change_down).over(window)).drop(df.change_down)\n",
    "  df = df.withColumn(\"rsi\", F.round((100 * df.avg_up / (df.avg_up + F.abs(df.avg_down))), 4))\n",
    "  df = df.fillna(0)\n",
    "  return df.rdd.map(lambda x: x.rsi).collect()\n",
    "\n",
    "def calcVPT(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"temp_vpt\", F.when(F.isnull((df.Volume * (df.Close - df.prev_close))/df.prev_close), 0).otherwise((df.Volume * (df.Close - df.prev_close))/df.prev_close))\n",
    "  df = df.withColumn(\"prev_vpt\", F.lag(df.temp_vpt).over(window))\n",
    "  df = df.withColumn(\"vpt\", F.when(F.isnull( df.prev_vpt), df.temp_vpt).otherwise(df.temp_vpt + df.prev_vpt)).drop(df.temp_vpt).drop(df.prev_vpt)\n",
    "  return df.rdd.map(lambda x: x.vpt).collect()\n",
    "\n",
    "def getList(df, col_name, double=True): \n",
    "  rows = df.select(F.col(col_name)).collect()\n",
    "  if double: \n",
    "    return [float(row[0]) for row in rows]\n",
    "  return [row[0] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['34.148.39.178']) # replace localhost with your Cassandra host IP\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create keyspace \n",
    "session.execute(\"CREATE KEYSPACE main WITH replication = {'class':'SimpleStrategy', 'replication_factor': 3};\")\n",
    "\n",
    "# Create stock table\n",
    "session.execute(\"CREATE TABLE main.stock(TICKER text, SMA_10 LIST<double>, SMA_50 LIST<double>, SMA_100 LIST<double>, DAILY_PERCENTAGE LIST<double>, ATR LIST<double>, VPT LIST<double>, RSI LIST<double>, DATES LIST<date>, OPEN LIST<double>, CLOSE LIST<double>, LOW LIST<double>, HIGH LIST<double>, PRIMARY KEY(TICKER));\")\n",
    "\n",
    "# Create prepared statement\n",
    "insert_stock = session.prepare(\"\"\"\n",
    "    INSERT INTO main.stock (TICKER, SMA_10, SMA_50, SMA_100, DAILY_PERCENTAGE, ATR, VPT, RSI, DATES, OPEN, CLOSE, LOW, HIGH) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeCalc(df): \n",
    "  data = [df.collect()[0][0], calcSimpleMovingAvg(df, 'Close', 10),calcSimpleMovingAvg(df, 'Close', 50),calcSimpleMovingAvg(df, 'Close', 100),calcDailyPercentChange(df), calcATR(df), calcVPT(df), calcRSI(df), getList(df, \"Date\", False), getList(df, \"Open\"), getList(df, \"Close\"),getList(df, \"Low\"), getList(df, \"High\")]\n",
    "  session.execute(insert_stock, (data[0], data[1], data[2], data[3], data[4], data[5], data[6], data[7],data[8],data[9],data[10],data[11],data[12]))\n",
    "  print(data[0] + \" data entered into Cassandra \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "tickers = df.select(col(\"Ticker\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "stock_dfs = [df.where(df[\"Ticker\"] == ticker) for ticker in tickers]\n",
    "\n",
    "for df in stock_dfs:\n",
    "  executeCalc(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
