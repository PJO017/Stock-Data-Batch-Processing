{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Stock-Data-Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PJ-Ubuntu.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Stock-Data-Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f97001f9c30>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset \n",
    "df = spark.read.option('header', 'true').csv(\"./Data/FS_sp500_Value.csv\").drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis functions \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def calcSimpleMovingAvg(df, col, span):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-span, 0)\n",
    "  df = df.withColumn(\"moving_avg\", F.avg(col).over(window))\n",
    "\n",
    "  return df.rdd.map(lambda x: x.moving_avg).collect()\n",
    "\n",
    "def calcDailyPercentChange(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"change\", F.when(F.isnull( (df.Close - df.prev_close)/df.prev_close ), 0).otherwise(F.round(F.abs((df.Close - df.prev_close)/df.prev_close) *100, 3)) )\n",
    "\n",
    "  return df.rdd.map(lambda x: x.change).collect()\n",
    "\n",
    "def calcATR(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"h-l\", df.High-df.Low)\n",
    "  df = df.withColumn(\"h-p\", F.when(F.isnull( F.abs(df.High-df.prev_close)), 0).otherwise( F.abs(df.High-df.prev_close)))\n",
    "  df = df.withColumn(\"l-p\", F.when(F.isnull( F.abs(df.Low-df.prev_close)), 0).otherwise( F.abs(df.Low-df.prev_close)))\n",
    "  df = df.withColumn(\"true_range\", F.greatest(\"h-l\", \"h-p\", \"l-p\"))\n",
    "\n",
    "  return calcSimpleMovingAvg(df, \"true_range\", 14)\n",
    "\n",
    "def calcRSI(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window)) \n",
    "  df = df.withColumn(\"change\", F.when(F.isnull(df.Close - df.prev_close), 0).otherwise(df.Close - df.prev_close))\n",
    "\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-14, 0)\n",
    "  df = df.withColumn(\"change_up\", F.when(df.change < 0, 0).otherwise(df.change))\n",
    "  df = df.withColumn(\"change_down\", F.when(df.change > 0, 0).otherwise(df.change))\n",
    "  df = df.withColumn(\"avg_up\", F.avg(df.change_up).over(window)).drop(df.change_up)\n",
    "  df = df.withColumn(\"avg_down\", F.avg(df.change_down).over(window)).drop(df.change_down)\n",
    "  df = df.withColumn(\"rsi\", F.round((100 * df.avg_up / (df.avg_up + F.abs(df.avg_down))), 4))\n",
    "  df = df.fillna(0)\n",
    "  return df.rdd.map(lambda x: x.rsi).collect()\n",
    "\n",
    "def calcVPT(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"temp_vpt\", F.when(F.isnull((df.Volume * (df.Close - df.prev_close))/df.prev_close), 0).otherwise((df.Volume * (df.Close - df.prev_close))/df.prev_close))\n",
    "  df = df.withColumn(\"prev_vpt\", F.lag(df.temp_vpt).over(window))\n",
    "  df = df.withColumn(\"vpt\", F.when(F.isnull( df.prev_vpt), df.temp_vpt).otherwise(df.temp_vpt + df.prev_vpt)).drop(df.temp_vpt).drop(df.prev_vpt)\n",
    "  return df.rdd.map(lambda x: x.vpt).collect()\n",
    "\n",
    "def getDates(df): \n",
    "  return df.rdd.map(lambda x: x.Date).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['localhost'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['localhost']) # replace localhost with your Cassandra host IP\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create keyspace \n",
    "session.execute(\"CREATE KEYSPACE main WITH replication = {'class':'SimpleStrategy', 'replication_factor': 1};\")\n",
    "\n",
    "# Create stock table\n",
    "session.execute(\"CREATE TABLE main.stock(TICKER text, MOVING_AVG LIST<double>, DAILY_PERCENTAGE LIST<double>, ATR LIST<double>, VPT LIST<double>, RSI LIST<double>, DATES LIST<date>, PRIMARY KEY(TICKER));\")\n",
    "\n",
    "# Create prepared statement\n",
    "insert_stock = session.prepare(\"\"\"\n",
    "    INSERT INTO main.stock (TICKER, MOVING_AVG, DAILY_PERCENTAGE, ATR, VPT, RSI, DATES) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeCalc(df): \n",
    "  data = [df.collect()[0][0], calcSimpleMovingAvg(df, 'Close', 14),calcDailyPercentChange(df), calcATR(df), calcVPT(df), calcRSI(df), getDates(df)]\n",
    "  session.execute(insert_stock, (data[0], data[1], data[2], data[3], data[4], data[5], data[6]))\n",
    "  print(data[0] + \" data entered into Cassandra \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "tickers = df.select(col(\"Ticker\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "stock_dfs = [df.where(df[\"Ticker\"] == ticker) for ticker in tickers]\n",
    "\n",
    "# for df in stock_dfs:\n",
    "#   executeCalc(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
