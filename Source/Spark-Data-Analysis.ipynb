{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 00:01:52 WARN Utils: Your hostname, PJ-Ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.251 instead (on interface wlo1)\n",
      "23/04/15 00:01:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/15 00:01:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Stock-Data-Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.251:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Stock-Data-Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f742c38fbb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the dataset \n",
    "df = spark.read.option('header', 'true').csv(\"./Data/FS_sp500_Value.csv\").drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis functions \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def calcSimpleMovingAvg(df, col, span):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-span, 0)\n",
    "  df = df.withColumn(\"moving_avg\", F.avg(col).over(window))\n",
    "\n",
    "  return df.rdd.map(lambda x: x.moving_avg).collect()\n",
    "\n",
    "def calcDailyPercentChange(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"change\", F.when(F.isnull( (df.Close - df.prev_close)/df.prev_close ), 0).otherwise(F.round(F.abs((df.Close - df.prev_close)/df.prev_close) *100, 3)) )\n",
    "\n",
    "  return df.rdd.map(lambda x: x.change).collect()\n",
    "\n",
    "def calcATR(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"h-l\", df.High-df.Low)\n",
    "  df = df.withColumn(\"h-p\", F.when(F.isnull( F.abs(df.High-df.prev_close)), 0).otherwise( F.abs(df.High-df.prev_close)))\n",
    "  df = df.withColumn(\"l-p\", F.when(F.isnull( F.abs(df.Low-df.prev_close)), 0).otherwise( F.abs(df.Low-df.prev_close)))\n",
    "  df = df.withColumn(\"true_range\", F.greatest(\"h-l\", \"h-p\", \"l-p\"))\n",
    "\n",
    "  return calcSimpleMovingAvg(df, \"true_range\", 14)\n",
    "\n",
    "def calcRSI(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window)) \n",
    "  df = df.withColumn(\"change\", F.when(F.isnull(df.Close - df.prev_close), 0).otherwise(df.Close - df.prev_close))\n",
    "\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(-14, 0)\n",
    "  df = df.withColumn(\"change_up\", F.when(df.change < 0, 0).otherwise(df.change))\n",
    "  df = df.withColumn(\"change_down\", F.when(df.change > 0, 0).otherwise(df.change))\n",
    "  df = df.withColumn(\"avg_up\", F.avg(df.change_up).over(window)).drop(df.change_up)\n",
    "  df = df.withColumn(\"avg_down\", F.avg(df.change_down).over(window)).drop(df.change_down)\n",
    "  df = df.withColumn(\"rsi\", F.round((100 * df.avg_up / (df.avg_up + F.abs(df.avg_down))), 4))\n",
    "  df = df.fillna(0)\n",
    "  return df.rdd.map(lambda x: x.rsi).collect()\n",
    "\n",
    "def calcVPT(df):\n",
    "  window = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "  df = df.withColumn(\"prev_close\", F.lag(df.Close).over(window))\n",
    "  df = df.withColumn(\"temp_vpt\", F.when(F.isnull((df.Volume * (df.Close - df.prev_close))/df.prev_close), 0).otherwise((df.Volume * (df.Close - df.prev_close))/df.prev_close))\n",
    "  df = df.withColumn(\"prev_vpt\", F.lag(df.temp_vpt).over(window))\n",
    "  df = df.withColumn(\"vpt\", F.when(F.isnull( df.prev_vpt), df.temp_vpt).otherwise(df.temp_vpt + df.prev_vpt)).drop(df.temp_vpt).drop(df.prev_vpt)\n",
    "  return df.rdd.map(lambda x: x.vpt).collect()\n",
    "\n",
    "def getDates(df): \n",
    "  return df.rdd.map(lambda x: x.Date).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoHostAvailable",
     "evalue": "('Unable to connect to any servers', {'127.0.0.1:9042': ConnectionRefusedError(111, \"Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\")})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoHostAvailable\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcassandra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cluster\n\u001b[1;32m      3\u001b[0m cluster \u001b[38;5;241m=\u001b[39m Cluster([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# replace localhost with your Cassandra host IP\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mcluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create keyspace \u001b[39;00m\n\u001b[1;32m      7\u001b[0m session\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE KEYSPACE main WITH replication = \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimpleStrategy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplication_factor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 3};\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cassandra/cluster.py:1667\u001b[0m, in \u001b[0;36mcassandra.cluster.Cluster.connect\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cassandra/cluster.py:1703\u001b[0m, in \u001b[0;36mcassandra.cluster.Cluster.connect\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cassandra/cluster.py:1690\u001b[0m, in \u001b[0;36mcassandra.cluster.Cluster.connect\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cassandra/cluster.py:3488\u001b[0m, in \u001b[0;36mcassandra.cluster.ControlConnection.connect\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cassandra/cluster.py:3533\u001b[0m, in \u001b[0;36mcassandra.cluster.ControlConnection._reconnect_internal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNoHostAvailable\u001b[0m: ('Unable to connect to any servers', {'127.0.0.1:9042': ConnectionRefusedError(111, \"Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\")})"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['localhost']) # replace localhost with your Cassandra host IP\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create keyspace \n",
    "session.execute(\"CREATE KEYSPACE main WITH replication = {'class':'SimpleStrategy', 'replication_factor': 3};\")\n",
    "\n",
    "# Create stock table\n",
    "session.execute(\"CREATE TABLE main.stock(TICKER text, MOVING_AVG LIST<double>, DAILY_PERCENTAGE LIST<double>, ATR LIST<double>, VPT LIST<double>, RSI LIST<double>, DATES LIST<date>, PRIMARY KEY(TICKER));\")\n",
    "\n",
    "# Create prepared statement\n",
    "insert_stock = session.prepare(\"\"\"\n",
    "    INSERT INTO main.stock (TICKER, MOVING_AVG, DAILY_PERCENTAGE, ATR, VPT, RSI, DATES) \n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeCalc(df): \n",
    "  data = [df.collect()[0][0], calcSimpleMovingAvg(df, 'Close', 14),calcDailyPercentChange(df), calcATR(df), calcVPT(df), calcRSI(df), getDates(df)]\n",
    "  session.execute(insert_stock, (data[0], data[1], data[2], data[3], data[4], data[5], data[6]))\n",
    "  print(data[0] + \" data entered into Cassandra \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "tickers = df.select(col(\"Ticker\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "stock_dfs = [df.where(df[\"Ticker\"] == ticker) for ticker in tickers]\n",
    "\n",
    "for df in stock_dfs:\n",
    "  executeCalc(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
